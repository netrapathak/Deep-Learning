#THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python CNN.py

from __future__ import print_function
from keras.models import load_model
from keras.layers import Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten
from keras.optimizers import Adam, Adagrad, RMSprop
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import History, EarlyStopping, ModelCheckpoint, TensorBoard
from keras.models import Sequential
from keras.regularizers import l2
from keras.constraints import maxnorm
from keras.utils import np_utils
from keras import backend as K
import numpy as np
import os, h5py
from PIL import Image

TRAIN_DIR = '.././Training'
VALIDATION_DIR = '.././Validation'

# input image dimensions
img_width = 120
img_height = 30

# number of channels
img_channels = 1
nb_filters = 32
# size of pooling area for max pooling
nb_pool = 3
# convolution kernel size
nb_conv = 3

# fix random seed for reproducibility
seed = 7
np.random.seed(seed)

epochs = 200
batch_size = 400

#------------------------------------------------------------------------------

# % Convolution Model
def model(nb_filters, nb_conv, nb_pool, nb_classes, input_shape, data_format):
    model = Sequential()

    model.add(Convolution2D(nb_filters, nb_conv, padding='same', data_format=data_format, kernel_initializer='glorot_normal', input_shape=input_shape))
    model.add(Activation('relu'))
    model.add(Convolution2D((2*nb_filters), nb_conv, padding='same', kernel_initializer='glorot_normal'))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))
    model.add(Dropout(0.25))

    model.add(Convolution2D((2*nb_filters), nb_conv, padding='same', kernel_initializer='glorot_normal'))
    model.add(Activation('relu'))
    model.add(Convolution2D((3*nb_filters), nb_conv, padding='same', kernel_initializer='glorot_normal',kernel_regularizer=l2(0.01)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))
    model.add(Dropout(0.25))

    model.add(Convolution2D((3*nb_filters), nb_conv, padding='same', kernel_initializer='glorot_normal',kernel_regularizer=l2(0.01)))
    model.add(Activation('relu'))
    model.add(Convolution2D(4*nb_filters, nb_conv, padding='same', kernel_initializer='glorot_normal',kernel_regularizer=l2(0.01)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))
    model.add(Dropout(0.25))

    model.add(Flatten())
    model.add(Dense(512,kernel_constraint=maxnorm(3)))
    model.add(Activation('relu'))
    model.add(Dense(128,kernel_constraint=maxnorm(3)))
    model.add(Activation('relu'))
    model.add(Dropout(0.4))
    model.add(Dense(nb_classes))
    model.add(Activation('softmax'))

    print(model.summary())

    return model


nb_classes = len(os.listdir(TRAIN_DIR))
nb_train_samples = sum([len(files) for r, d, files in os.walk(TRAIN_DIR)])
nb_validation_samples = sum([len(files) for r, d, files in os.walk(VALIDATION_DIR)])

train_datagen = ImageDataGenerator(
    rescale=1. / 255,
    shear_range=0.2,
    zoom_range=0.2,
    featurewise_center=True,
    featurewise_std_normalization=True)

test_datagen = ImageDataGenerator(rescale=1. / 255)

train_generator = train_datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    color_mode='grayscale',
    class_mode='binary')

validation_generator = test_datagen.flow_from_directory(
    VALIDATION_DIR,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    color_mode='grayscale',
    class_mode='binary')

if K.image_dim_ordering() == 'th':
    input_shape = (img_channels, img_width, img_height)
    data_format = "channels_first"
else:
    input_shape = (img_width, img_height, img_channels)
    data_format = "channels_last"

model = model(nb_filters, nb_conv, nb_pool, nb_classes, input_shape, data_format)

# model = load_model('model.h5')

model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=["accuracy"])
model_filename = 'model.h5'

earlystop = EarlyStopping(monitor='val_acc', patience=50, verbose=1, mode='auto')
check = ModelCheckpoint(model_filename,
                        monitor='val_acc',
                        verbose=1,
                        save_best_only=True,
                        mode='auto')
history = History()
tensorboard = TensorBoard(log_dir='./logs', write_graph=True, write_images=True)

# Fit the model on the batches generated by datagen.flow().
model.fit_generator(train_generator,
                    steps_per_epoch=(nb_train_samples/batch_size),
                    epochs=epochs,
                    validation_data=validation_generator,
                    validation_steps=(nb_validation_samples/batch_size),
                    callbacks=[earlystop,check,history,tensorboard],
                    verbose=1)

# serialize model to JSON
model_json = model.to_json()
with open('model.json', "w") as json_file:
    json_file.write(model_json)
